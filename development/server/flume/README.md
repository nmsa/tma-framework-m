
# Apache Flume
Apache Flume is a distributed, available and reliable tool to efficiently collect, aggregate and move large amounts of log data. Apache Flume has a simple and flexible architecture based on message flows. This tool has fault tolerant, failover and recovery mechanisms.

## Prerequisites
To use Apache Flume, you need to initialize the Kubernetes cluster and deploy on it all components of server folder of this repository following the instructions present in README file of that folder.

## Installation
Apache Flume is already installed in Apache Kafka image that is used to deploy it in Kubernetes. So, for initialize Apache Flume you need to run flume.sh script. To do that execute the following command:
```
./flume.sh
```
This script shows a menu with two options. One of the options configures and executes Apache Flume to save the received data in a MySQL database that belongs to TMA_Knowledge component.
For every observation in data received, one row in the database is generated with the following format:
probeId, resourceId,  type, descriptionId, time,value
Each of these fields are saved in one column in table created in database.
The second option presented in the menu of flume.sh script configures the Testing Mode in Apache Flume. In this mode, Apache Flume save in a log file, for each observation, a SQL query to make possible user inserts it in all types of SQL databases.
## Testing
For testing purposes, you need to deploy all components of TMA_Monitor and one probe that generates valid data.
After those deployments, if you choose the option of save data in MySQL, you just need to check the database content and you will see the data formatted and stored there.
To test Apache Flume in Testing Mode you just need to check the respective log file in testingmode desktop folder on the node of the Kubernetes cluster that Apache Kafka pod is running. In testing mode, the throughput of the data generated by probes are not supported by Apache Flume, so to test this mode you can use the script presented in test folder of this repository.
After running this script, you can check the log file with the queries ready to insert in any SQL database. 
